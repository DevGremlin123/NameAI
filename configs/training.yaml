# Training configuration — optimized for RTX 5090 (32GB VRAM)
# Two-phase: (1) Wikipedia MLM pre-training, (2) name generation fine-tuning

bpe_model: data/bpe_tokenizer.model

# Phase 1: Pre-train encoder on Wikipedia (learns English)
pretrain:
  wiki_text: data/wikipedia/wiki_text.txt
  epochs: 3
  batch_size: 256              # ~8GB with 512 seq len in BF16
  learning_rate: 1.0e-3
  max_len: 512
  save_path: checkpoints/pretrained.pt

# Phase 2: Fine-tune full model on (description → brand name) pairs
finetune:
  pretrained: checkpoints/pretrained.pt
  train_data: data/processed/train.jsonl
  val_data: data/processed/val.jsonl
  epochs: 20
  batch_size: 256              # Names are short, fits easily
  learning_rate: 5.0e-4
  save_dir: checkpoints

optimizer:
  weight_decay: 0.01
  betas: [0.9, 0.98]
  max_grad_norm: 1.0

scheduler:
  warmup_ratio: 0.1           # 10% of total steps
  type: cosine                 # Cosine decay to 1% of peak LR

precision:
  bf16: true                   # Native on 5090 / B200

logging:
  wandb_project: nameai
